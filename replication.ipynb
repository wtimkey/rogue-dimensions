{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8202e96",
   "metadata": {},
   "source": [
    "# All Bark and No Bite: Rogue Dimensions in Transformer Language Models Obscure Representational Quality\n",
    "\n",
    "### Replication Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593f4412",
   "metadata": {},
   "source": [
    "## 1) Build a corpus\n",
    "Before we do anything, we need a corpus of natural language data. We used randomly selected dumps from wikipedia, but a dataset such as [wikitext-103](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/) should suffice for most experiments. For the final experiment (Section 5), you will need to use a larger corpus, hence our use of several dump partitions. Otherwise, any sufficently large corpus should give the same qualitative results\n",
    "\n",
    "We acquired our wikipedia files from: https://dumps.wikimedia.org/enwiki/latest/. The specific partitions we used are:\n",
    "enwiki-latest-pages-articles[X].xml where X is 4, 5, 7, 8, 10, 11, 12, 13, 15, 16, 18, 20, 22, 24, 25.\n",
    "\n",
    "If you used raw wikipedia dumps, then you'll need to use [WikiExtractor](https://github.com/attardi/wikiextractor) to extract and clean the text.\n",
    "\n",
    "Save your wikipedia dumps to a folder in the same directory called \"wikitext/\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bd6ba0",
   "metadata": {},
   "source": [
    "## 2) Preprocessing\n",
    "Now, we'll get embeddings from all of the models using our corpus. Feel free to add any other language models you'd like to investigate from https://huggingface.co/models using the model identifier. If you'd like to skip this step and use the embeddings we used in the paper, you can find them [here](https://drive.google.com/file/d/1YUgEHEu6QU2ChD0X9N8IlZqNJlr3rk0P/view?usp=sharing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cf0c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import sys\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "random.seed(2398)\n",
    "\n",
    "if torch.cuda.is_available():  \n",
    "    dev = \"cuda:0\"\n",
    "    sys.stdout.write('using CUDA!\\n')\n",
    "else:\n",
    "    sys.stdout.write('NOT using CUDA!\\n')\n",
    "    dev = \"cpu\"  \n",
    "device = torch.device(dev)\n",
    "\n",
    "\n",
    "def load_model_tokenizer(model_name):\n",
    "  model = AutoModel.from_pretrained(model_name, output_hidden_states=True)\n",
    "  model.to(device)\n",
    "  model.eval()\n",
    "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "  return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e22411",
   "metadata": {},
   "outputs": [],
   "source": [
    "#these are the four models we use in our experiments \n",
    "#you can add/change models, though some of the code is dependent on all models having the same # of layers.\n",
    "models = ['gpt2', 'xlnet-base-cased', 'bert-base-cased', 'roberta-base']\n",
    "\n",
    "#choose a model, or add a for loop to get embeddings for all models.\n",
    "model_name = models[0]\n",
    "model, tokenizer = load_model_tokenizer(model_name)\n",
    "\n",
    "#context length is the number of tokens in each model input. In our case, we chose 128:\n",
    "context_length = 128\n",
    "\n",
    "wikitext_fnames = glob('wikitext/*/*/*')\n",
    "random.shuffle(wikitext_fnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd6ee39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocesssing the dump files to ignore headers, spacing etc. You can ignore this if you're not using wiki dumps\n",
    "all_article_texts = []\n",
    "for fname in wikitext_fnames[:100]:\n",
    "    with open(fname, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    in_article = False\n",
    "    article_texts = []\n",
    "    curr_article = []\n",
    "    i = 0\n",
    "    while(i < len(lines)):\n",
    "        if(len(lines[i]) > 1):\n",
    "            if(in_article):\n",
    "                if(lines[i][:6] != '</doc>'):\n",
    "                    curr_article.append(\" \".join(lines[i].strip().split()))\n",
    "                else:\n",
    "                    in_article = False\n",
    "                    if(len(curr_article) > 10):\n",
    "                        article_texts.append(\" \".join(curr_article))\n",
    "                    curr_article = []\n",
    "            elif(lines[i][:4] == '<doc'):\n",
    "                in_article = True\n",
    "                i+=1\n",
    "        i+=1\n",
    "    all_article_texts.extend(article_texts)\n",
    "    \n",
    "random.shuffle(all_article_texts)\n",
    "texts = all_article_texts[:150] #for this part, we don't need all of the article text, so lets just take a random subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac70de8d",
   "metadata": {},
   "source": [
    "The next cell is where we actually get the embeddings from samples in our corpus. If you are using a different corpus, then create a list of ```str``` paragraphs called ```texts``` with whatever text you'd like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8606f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_sample_embeds = []\n",
    "all_toks = []\n",
    "for text in tqdm(texts):\n",
    "    text_toks = tokenizer.tokenize(text)[:context_length]\n",
    "    all_toks += text_toks\n",
    "    toks = torch.tensor([tokenizer.convert_tokens_to_ids(text_toks)]).to(device)\n",
    "    model_out = model(toks)\n",
    "    embeddings = torch.stack(model_out.hidden_states).squeeze().detach().cpu().numpy()\n",
    "    corpus_sample_embeds.append(embeddings)\n",
    "\n",
    "corpus_sample_embeds = np.concatenate(corpus_sample_embeds, axis=1)\n",
    "\n",
    "out = {'corpus':texts, 'tokens':all_toks, 'embeddings':corpus_sample_embeds}\n",
    "directory = 'wikitext_embs'\n",
    "if not os.path.exists(directory):\n",
    "os.makedirs(directory)\n",
    "pickle.dump(out, open(os.path.join(directory, model_name + \"_\" + str(context_length) + '.p'), \"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4581b789",
   "metadata": {},
   "source": [
    "## 2.5) Context Aggregation for representational quality evaluation (section 5)\n",
    "If you would like to replicate our results directly from the wiki corpus, then you'll need to aggregate across corpora using the strategy of [Bommasani et al. 2020](https://aclanthology.org/2020.acl-main.431.pdf). To save space, we've included all of the aggregated contexts for all words in all of the human word similarity/relatedness datasets. Our data can be found [here](https://drive.google.com/file/d/1_1wQGXSmgjnKXmKpHaLopLf0A1_6gwo6/view?usp=sharing). Extract these files to the ```data/``` directory. \n",
    "\n",
    "Otherwise, if you'd just like to use our precomputed context-aggregated embeddings, they are availible [here](https://drive.google.com/file/d/1AenKhgoJcvzwaAqgCchNkR55Q3uiYdPA/view?usp=sharing).\n",
    "\n",
    "If you'd like to use your own corpus, then format the files as follows:\n",
    "each file is named ```fname.sents```, where ```fname``` is the name of the target word from the word similarity dataset. Each file has line seperated sentences including the target word, where the first element of each line is the index of the target word in the sentence. All words are space-delimited. \n",
    "\n",
    "The word similarity/relatedness datasets can be found in the github repository for our paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f76318",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_filenames = ['word_sim/simlex-999/SimLex-999.txt', \n",
    "                'word_sim/simverb-3500/SimVerb-3500.txt', \n",
    "                'word_sim/wordsim-353_r/wordsim_relatedness_goldstandard.txt', \n",
    "                'word_sim/wordsim-353_s/wordsim_similarity_goldstandard.txt',\n",
    "                'word_sim/rg-65/RG65.txt']\n",
    "\n",
    "#load all of the vocabulary items from all of the word similarity datasets\n",
    "def load_sim_dataset_vocab(sim_filenames):\n",
    "    sim_lex_vocab = set()\n",
    "    sim_lex_pairs = []\n",
    "    for sim_lex_filename in sim_filenames:\n",
    "        with open(sim_lex_filename, 'r', encoding='utf-8') as f:\n",
    "            dataset = sim_lex_filename.split('/')[0]\n",
    "            word_pair_lines = f.readlines()\n",
    "            for line in word_pair_lines[1:]:\n",
    "                if(sim_lex_filename[:2] == 'rg'):\n",
    "                    vals = line.split(';')\n",
    "                else:\n",
    "                    vals = line.split()\n",
    "                word1 = vals[0].lower()\n",
    "                word2 = vals[1].lower()\n",
    "                sim_lex_vocab.add(word1)\n",
    "                sim_lex_vocab.add(word2)\n",
    "                sim_lex_pairs.append((word1, word2, dataset))\n",
    "    return sim_lex_vocab, sim_lex_pairs\n",
    "\n",
    "#tokenize words using BPE, such that we can recover the indicies of the target word.\n",
    "def tokenize_sent_subword_inds(sent, target_word_ind, tokenizer):\n",
    "    final_sent_toks = []\n",
    "    start_ind = 0\n",
    "    end_ind = 0\n",
    "    for i, word in enumerate(sent):\n",
    "        if i != 0:\n",
    "            word = \" \" + word #add prefix space for intermediate words\n",
    "        curr_word_toks = tokenizer.tokenize(word)\n",
    "        if i == target_word_ind:\n",
    "            start_ind = len(final_sent_toks)\n",
    "            final_sent_toks.extend(curr_word_toks)\n",
    "            end_ind = len(final_sent_toks)\n",
    "        else:\n",
    "            final_sent_toks.extend(curr_word_toks)\n",
    "    return final_sent_toks, (start_ind, end_ind)\n",
    "\n",
    "def get_embeddings_for_word(model, tokenizer, target_word, target_words_dir):\n",
    "    with open(os.path.join(target_words_dir, target_word) + \".sents\", 'r', encoding='utf-8') as f:\n",
    "        f_lines = f.readlines()\n",
    "        sents = []\n",
    "    for line in f_lines:\n",
    "        target_word_ind = int(line.split()[0])\n",
    "        sent = line.split()[1:]\n",
    "        sents.append((target_word_ind, sent))\n",
    "  #get embeddings\n",
    "    mean_embs = []\n",
    "    for sent in sents[:500]: #no more than 500 sentences for time considerations\n",
    "        tokens, target_span = tokenize_sent_subword_inds(sent[1], sent[0], tokenizer)\n",
    "        tokens_tensor = torch.tensor([tokenizer.convert_tokens_to_ids(tokens)]).to(device)\n",
    "        with torch.no_grad():\n",
    "            model_out = model(tokens_tensor)\n",
    "            embeddings = torch.stack(model_out.hidden_states).squeeze()[:,target_span[0]:target_span[1],:]\n",
    "            embeddings = embeddings.detach().cpu().numpy()\n",
    "        mean_embedding = embeddings.mean(axis=1) #average over subwords\n",
    "        mean_embs.append(mean_embedding)\n",
    "    decontextualized_emb = np.stack(mean_embs).mean(axis=0)\n",
    "    return decontextualized_emb\n",
    "\n",
    "  \n",
    "vocab, pairs = load_sim_dataset_vocab(sim_filenames)\n",
    "vocab_processed = set()\n",
    "#model, tokenizer = load_model_tokenizer(model_name)\n",
    "for word_i in tqdm(vocab):\n",
    "    mean_emb = get_embeddings_for_word(model, tokenizer, word_i, 'data')\n",
    "  \n",
    "    directory = 'wordsim_embs/{}'.format(model_name)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    if(mean_emb is not None and word_i not in vocab_processed):\n",
    "        vocab_processed.add(word_i)\n",
    "        pickle.dump(mean_emb, open(os.path.join(directory, '{}.p'.format(word_i)), \"wb\" ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27890e7a",
   "metadata": {},
   "source": [
    "Great - preprocessing complete! Now for some analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79280f6",
   "metadata": {},
   "source": [
    "## 3) Reproducing Section 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0da85e",
   "metadata": {},
   "source": [
    "### Section 3.1\n",
    "Now we'll reproduce the results in section 3.1. This section examines the cosine sim. contribution of each dimension of each layer of the models. The final cell will print the results table in latex format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d237b874",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports, some of these might be replicates\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "from transformers import AutoModelForCausalLM, AutoModelForMaskedLM, AutoTokenizer\n",
    "import torch\n",
    "import sys\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy import stats\n",
    "from matplotlib import pyplot as plt\n",
    "random.seed(2398)\n",
    "\n",
    "if torch.cuda.is_available():  \n",
    "    dev = \"cuda:0\"\n",
    "    sys.stdout.write('using CUDA!\\n')\n",
    "else:\n",
    "    sys.stdout.write('NOT using CUDA!\\n')\n",
    "    dev = \"cpu\"  \n",
    "device = torch.device(dev)\n",
    "\n",
    "\n",
    "models = ['gpt2', 'xlnet-base-cased', 'bert-base-cased', 'roberta-base']\n",
    "\n",
    "num_layers = {'gpt2':13, 'xlnet-base-cased':13, 'bert-base-cased':13, 'bert-base-uncased':13, 'roberta-base':13}\n",
    "num_dims = {'gpt2':768, 'xlnet-base-cased':768, 'bert-base-cased':768, 'bert-base-uncased':768, 'roberta-base':768}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bcb3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the cosine contribution function we define in eq. 3\n",
    "def cos_contrib(emb1, emb2):\n",
    "    numerator_terms = emb1 * emb2\n",
    "    denom = np.linalg.norm(emb1) * np.linalg.norm(emb2)\n",
    "    return numerator_terms / denom "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ceb4065",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the experiments!\n",
    "cd_s = {}\n",
    "for model_name in models:\n",
    "    print(f'{model_name} Cosine sim contribution analysis:')\n",
    "    sample_output = pickle.load(open(\"wikitext_embs/{}_128.p\".format(model_name), \"rb\" ))\n",
    "    sample_data = sample_output['embeddings']\n",
    "    rogue_dist = []\n",
    "    num_toks = sample_data.shape[1]\n",
    "    \n",
    "    #randomly sample embedding pairs to compute avg. cosine similiarity contribution\n",
    "    random_pairs = [random.sample(range(num_toks), 2) for i in range(500000)]\n",
    "    \n",
    "    cos_contribs_by_layer = []\n",
    "    for layer in range(num_layers[model_name]):\n",
    "        layer_cosine_contribs = []\n",
    "        layer_rogue_cos_contribs = []\n",
    "        for pair in random_pairs:\n",
    "            emb1 = sample_data[layer,pair[0],:]\n",
    "            emb2 = sample_data[layer,pair[1],:]\n",
    "            layer_cosine_contribs.append(cos_contrib(emb1, emb2))\n",
    "\n",
    "        layer_cosine_contribs = np.array(layer_cosine_contribs)\n",
    "        layer_cosine_sims = layer_cosine_contribs.sum(axis=1)\n",
    "        layer_cosine_contribs_mean = layer_cosine_contribs.mean(axis=0)\n",
    "        cos_contribs_by_layer.append(layer_cosine_contribs_mean)\n",
    "    cos_contribs_by_layer = np.array(cos_contribs_by_layer)\n",
    "    \n",
    "    aniso = cos_contribs_by_layer.sum(axis=1) #total anisotropy, measured as avg. cosine sim between random emb. pairs\n",
    "    for layer in range(num_layers[model_name]):\n",
    "        top_3_dims = np.argsort(cos_contribs_by_layer[layer])[-3:]\n",
    "        top = cos_contribs_by_layer[layer,top_3_dims[2]] / aniso[layer]\n",
    "        second = cos_contribs_by_layer[layer,top_3_dims[1]] / aniso[layer]\n",
    "        third = cos_contribs_by_layer[layer,top_3_dims[0]] / aniso[layer]\n",
    "        print(\"& {} & {:.3f} & {:.3f} & {:.3f} & {:.3f} \\\\\\\\\".format(layer, top, second, third, aniso[layer]))\n",
    "    \n",
    "    #save cos_contribs for later analyses\n",
    "    pickle.dump(cos_contribs_by_layer, open('{}_cos_contrib.p'.format(model_name), \"wb\" ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab7e945",
   "metadata": {},
   "source": [
    "Word2Vec and GloVe baselines\n",
    "\n",
    "1) download Word2Vec and GloVe models used in the paper:\n",
    "\n",
    "    Word2Vec: https://zenodo.org/record/4421380\n",
    "    \n",
    "    GloVe: https://nlp.stanford.edu/projects/glove/ (Wikipedia+Gigaword 5, 300d)\n",
    "    \n",
    "    \n",
    "2) place each .txt (glove) and .bin (word2vec) file in the folder ```static_embs/```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec1a192",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "#run above imports if running these cells from scratch\n",
    "def get_cos_contrib(vocab_list, vec_dict):\n",
    "    vocab_pairs_to_sample = [random.sample(vocab_list, 2) for i in range(500000)]\n",
    "    cos_contribs = []\n",
    "    \n",
    "    for pair in vocab_pairs_to_sample:\n",
    "        w1 = vec_dict[pair[0]]\n",
    "        w2 = vec_dict[pair[1]]\n",
    "        cos_contribs.append(cos_contrib(w1, w2))\n",
    "    \n",
    "    cos_contribs = np.array(cos_contribs)\n",
    "    cosine_sims = cos_contribs.sum(axis=1)\n",
    "    cosine_contribs_mean = cos_contribs.mean(axis=0)\n",
    "    aniso_estimate = cosine_sims.mean()\n",
    "    top_3_dims = np.sort(np.abs(cosine_contribs_mean))[-3:]\n",
    "    top_dim_inds = np.argsort(np.abs(cosine_contribs_mean))\n",
    "    return top_3_dims, aniso_estimate, top_dim_inds, cos_contribs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0aab39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get word2vec results\n",
    "w2v_model = gensim.models.KeyedVectors.load_word2vec_format('static_embs/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "w2v_vocab = list(w2v_model.vocab.keys())\n",
    "_, _, _, w2v_cos_contribs = get_cos_contrib(w2v_vocab, w2v_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae431c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get glove results\n",
    "\n",
    "#load model\n",
    "with open('static_embs/glove.6B.300d.txt', 'r', encoding='utf-8') as f:\n",
    "    glove = {x[0]:np.array(x[1:]).astype('float32') for x in [y.split() for y in f.readlines()]}\n",
    "\n",
    "glove_vocab = list(glove.keys())\n",
    "_, _, _, glove_cos_contribs = get_cos_contrib(glove_vocab, glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e2f79c",
   "metadata": {},
   "source": [
    "### Section 3.2\n",
    "The following section reproduces results from section 3.2, where we analyse the variance explained by the full d dimensional embedding space and a d-k dimensional embedding space, where each of the k dimensions is a \"rogue\" dimensions. Run the following cell to get results for cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1b7019",
   "metadata": {},
   "outputs": [],
   "source": [
    "contribs = {}\n",
    "for model_name in models:\n",
    "    contribs[model_name] = np.zeros((num_layers[model_name], 3))\n",
    "    print(f'{model_name} cosine informativity analysis:')\n",
    "    sample_output = pickle.load(open(\"wikitext_embs/{}_128.p\".format(model_name), \"rb\" ))\n",
    "    sample_data = sample_output['embeddings']\n",
    "  \n",
    "    rogue_dist = []\n",
    "    num_toks = sample_data.shape[1]\n",
    "    random_pairs = [random.sample(range(num_toks), 2) for i in range(100000)]\n",
    "    for layer in range(num_layers[model_name]):\n",
    "        layer_sims = []\n",
    "        layer_cos_contrib = []\n",
    "        for pair in random_pairs:\n",
    "            emb1 = sample_data[layer,pair[0],:]\n",
    "            emb2 = sample_data[layer,pair[1],:]\n",
    "            layer_sims.append(1 - cosine(emb1, emb2))\n",
    "            layer_cos_contrib.append(cos_contrib(emb1, emb2))\n",
    "    mean_cos_contrib = np.array(layer_cos_contrib).mean(axis=0)\n",
    "    for i, top_n in enumerate([1,3,5]):\n",
    "        no_top_dim_sim = []\n",
    "        botton_n_dims = np.argsort(mean_cos_contrib)[:-top_n]\n",
    "        for pair in random_pairs:\n",
    "            emb1_nonrogue = sample_data[layer,pair[0],botton_n_dims]\n",
    "            emb2_nonrogue = sample_data[layer,pair[1],botton_n_dims]\n",
    "            no_top_dim_sim.append(1 - cosine(emb1_nonrogue, emb2_nonrogue))\n",
    "        l2_var_explained = stats.pearsonr(layer_sims, no_top_dim_sim)[0]**2\n",
    "        contribs[model_name][layer,i] = l2_var_explained\n",
    "    print(f'& {layer} & {contribs[model_name][layer,0]:.3f} &  {contribs[model_name][layer,1]:.3f} &  {contribs[model_name][layer,1]:.3f} \\\\\\\\')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd53746d",
   "metadata": {},
   "source": [
    "Run this cell to replicate the results for the same experiments with Euclidean distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b240749",
   "metadata": {},
   "outputs": [],
   "source": [
    "contribs = {}\n",
    "for model_name in models:\n",
    "    contribs[model_name] = np.zeros((num_layers[model_name], 3))\n",
    "    print(f'{model_name} l2 informativity analysis:')\n",
    "    sample_output = pickle.load(open(\"wikitext_embs/{}_128.p\".format(model_name), \"rb\" ))\n",
    "    sample_data = sample_output['embeddings']\n",
    "    rogue_dist = []\n",
    "    num_toks = sample_data.shape[1]\n",
    "    random_pairs = [random.sample(range(num_toks), 2) for i in range(100000)]\n",
    "    for layer in range(num_layers[model_name]):\n",
    "        layer_distances = []\n",
    "        layer_dist_by_dim = []\n",
    "    for pair in random_pairs:\n",
    "        emb1 = sample_data[layer,pair[0],:]\n",
    "        emb2 = sample_data[layer,pair[1],:]\n",
    "        layer_distances.append(np.linalg.norm(emb1 - emb2))\n",
    "        layer_dist_by_dim.append(np.abs(emb1-emb2))\n",
    "    mean_dist_by_dim = np.array(layer_dist_by_dim).mean(axis=0)\n",
    "    for i, top_n in enumerate([1,3,5]):\n",
    "        no_top_dim_sim = []\n",
    "        botton_n_dims = np.argsort(mean_dist_by_dim)[:-top_n]\n",
    "        for pair in random_pairs:\n",
    "            emb1_nonrogue = sample_data[layer,pair[0],botton_n_dims]\n",
    "            emb2_nonrogue = sample_data[layer,pair[1],botton_n_dims]\n",
    "            no_top_dim_sim.append(np.linalg.norm(emb1_nonrogue - emb2_nonrogue))\n",
    "        l2_var_explained = stats.pearsonr(layer_distances, no_top_dim_sim)[0]**2\n",
    "        contribs[model_name][layer,i] = l2_var_explained\n",
    "    print(f'& {layer} & {contribs[model_name][layer,0]:.3f} &  {contribs[model_name][layer,1]:.3f} &  {contribs[model_name][layer,1]:.3f} \\\\\\\\')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3886a2d",
   "metadata": {},
   "source": [
    "Word2Vec and GloVe baselines\n",
    "\n",
    "(run the cell with the definition of ```get_cos_contrib``` above at least once before running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52372f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cos_informativity(vocab_list, vec_dict):\n",
    "    vocab_pairs_to_sample = [random.sample(vocab_list, 2) for i in range(500000)]\n",
    "    _,_,top_dims = get_cos_contrib(vocab_list, vec_dict)\n",
    "    var_explained = []\n",
    "    for dims_to_remove in [1,3,5]:\n",
    "        orig_cos_sims = []\n",
    "        dim_rm_cos_sims = []\n",
    "        for pair in vocab_pairs_to_sample:\n",
    "            w1 = vec_dict[pair[0]].copy()\n",
    "            w2 = vec_dict[pair[1]].copy()\n",
    "            orig_cos_sims.append(1-cosine(w1,w2))\n",
    "            w1[top_dims[-dims_to_remove:]] = 0\n",
    "            w2[top_dims[-dims_to_remove:]] = 0\n",
    "            dim_rm_cos_sims.append(1-cosine(w1,w2))\n",
    "        var_explained.append(stats.pearsonr(orig_cos_sims,dim_rm_cos_sims)[0]**2)\n",
    "    return var_explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adab35aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get word2vec results\n",
    "w2v_model = gensim.models.KeyedVectors.load_word2vec_format('static_embs/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "w2v_vocab = list(w2v_model.vocab.keys())\n",
    "print(get_cos_informativity(w2v_vocab, w2v_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4c4de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get GloVe results\n",
    "with open('static_embs/glove.6B.300d.txt', 'r', encoding='utf-8') as f:\n",
    "    glove = {x[0]:np.array(x[1:]).astype('float32') for x in [y.split() for y in f.readlines()]}\n",
    "print(get_cos_informativity(glove_vocab, glove))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e96292",
   "metadata": {},
   "source": [
    "## 4) Reproducing Section 4\n",
    "In this section, we use ablation experiments to quantify the influence of a single dimension on model behavior. This takes a very long time to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2211977",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('behavior influence analysis:')\n",
    "sm = torch.nn.Softmax(dim=1)\n",
    "layer_norm_names = {'gpt2':'transformer.h.{}.ln_1', #gpt-2 normalizes before components, so we have to zero out the first norm term of the next layer\n",
    "                    'xlnet-base-cased':'transformer.layer.{}.ff.layer_norm', \n",
    "                    'bert-base-cased':'bert.encoder.layer.{}.output.LayerNorm', \n",
    "                    'bert-base-uncased':'bert.encoder.layer.{}.output.LayerNorm', \n",
    "                    'roberta-base':'roberta.encoder.layer.{}.output.LayerNorm'}\n",
    "gpt_final_layer_norm_name = 'transformer.ln_f'\n",
    "\n",
    "# modifies layernorm by setting dimensions weights and biases to zero, \n",
    "# zeroing out that dimension as future input for ablation tests\n",
    "# returns a model with the modified parameters\n",
    "def remove_dim_from_model_params(layer, dimension, model, model_name):\n",
    "    model_copy = copy.deepcopy(model)\n",
    "    state_dict = model_copy.state_dict()\n",
    "\n",
    "    if(model_name == 'gpt2'):\n",
    "        if(layer == -1):\n",
    "            target_param_bias = 'transformer.ln_f.bias'\n",
    "            target_param_weight = 'transformer.ln_f.weight'\n",
    "        else:\n",
    "            target_param_bias = (layer_norm_names[model_name] + '.bias').format(num_layers[model_name] + layer)\n",
    "            target_param_weight = (layer_norm_names[model_name] + '.weight').format(num_layers[model_name] + layer)\n",
    "    else:    \n",
    "        target_param_bias = (layer_norm_names[model_name] + '.bias').format(num_layers[model_name] + layer - 1)\n",
    "        target_param_weight =  (layer_norm_names[model_name] + '.weight').format(num_layers[model_name] + layer - 1)\n",
    "\n",
    "    state_dict[target_param_bias][dimension] = 0.0\n",
    "    state_dict[target_param_weight][dimension] = 0.0\n",
    "\n",
    "    model_copy.load_state_dict(state_dict)\n",
    "    return model_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b30223",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize and make predictions!\n",
    "def tokenize_mlm(tokenizer, paragraphs):\n",
    "    paragraph_tokens = [tokenizer.tokenize(paragraph)[:128] for paragraph in paragraphs]\n",
    "    all_tok_ids = []\n",
    "    all_masked_toks = []\n",
    "    for tokens in paragraph_tokens:\n",
    "        num_toks_to_mask = int(len(tokens) * .15)\n",
    "        toks_to_mask = random.sample(range(len(tokens)), num_toks_to_mask) #mask\n",
    "        tok_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        for mask_ind in toks_to_mask:\n",
    "            tok_ids[mask_ind] = tokenizer.mask_token_id\n",
    "        all_tok_ids.append(tok_ids)\n",
    "        all_masked_toks.append(toks_to_mask)\n",
    "    return all_tok_ids, all_masked_toks\n",
    "\n",
    "\n",
    "def tokenize_clm(tokenizer, paragraphs):\n",
    "    return [tokenizer.encode(paragraph)[:128] for paragraph in paragraphs]\n",
    "\n",
    "#given a list of tokenized inputs, compute and return the output distributions (softmaxed logits) for next-word predictions in the model\n",
    "def make_predictions_clm(model, tokenizer, all_tok_ids, get_embs=False):\n",
    "    all_texts_dists = []\n",
    "    if(get_embs):\n",
    "        all_embs = []\n",
    "    else:\n",
    "        all_embs = None\n",
    "    for tok_ids in all_tok_ids:\n",
    "        tokens_tensor = torch.tensor([tok_ids]).to(device)\n",
    "        model_out = model(tokens_tensor)\n",
    "        preds = model_out.logits.squeeze()\n",
    "        if(get_embs):\n",
    "            embeddings = torch.stack(model_out.hidden_states).squeeze().detach().cpu().numpy()\n",
    "            all_embs.append(embeddings)\n",
    "        dists = sm(preds).detach().cpu().numpy()\n",
    "        all_texts_dists.append(dists)\n",
    "    all_texts_dists = np.concatenate(all_texts_dists, axis=0)\n",
    "    if(get_embs):\n",
    "        all_embs = np.concatenate(all_embs, axis=1)\n",
    "    return all_texts_dists, all_embs\n",
    "\n",
    "#given a list of tokenized inputs, compute and return the output distributions (softmaxed logits) for masked tokens in the model\n",
    "def make_predictions_mlm(model, tokenizer, all_tok_ids, all_masked_toks, get_embs=False):\n",
    "    all_texts_dists = []\n",
    "    if(get_embs):\n",
    "        all_embs = []\n",
    "    else:\n",
    "        all_embs = None\n",
    "    for i, tok_ids in enumerate(all_tok_ids):\n",
    "        toks_to_mask = all_masked_toks[i]\n",
    "        tokens_tensor = torch.tensor([tok_ids]).to(device)\n",
    "        model_out = model(tokens_tensor)\n",
    "        preds = model_out.logits.squeeze()[toks_to_mask,:]\n",
    "        if(get_embs):\n",
    "            embeddings = torch.stack(model_out.hidden_states).squeeze().detach().cpu().numpy()\n",
    "            embeddings = embeddings[:,toks_to_mask,:]\n",
    "            all_embs.append(embeddings)\n",
    "        dists = sm(preds).detach().cpu().numpy()\n",
    "        all_texts_dists.append(dists)\n",
    "    all_texts_dists = np.concatenate(all_texts_dists, axis=0)\n",
    "    if(get_embs):\n",
    "        all_embs = np.concatenate(all_embs, axis=1)\n",
    "    return all_texts_dists, all_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71deeedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute mean kl_d across all token distributions in the sample  \n",
    "def compute_avg_kl_d(model_distributions, ref_distributions):\n",
    "    all_tok_kl_d = entropy(ref_distributions, qk=model_distributions, axis=1)\n",
    "    return all_tok_kl_d.mean(), all_tok_kl_d.std()\n",
    "\n",
    "#load the appropriate type of language modeling head for the given model, additionally return whether the model does masked lm\n",
    "def load_model_tokenizer(model_name):\n",
    "    if model_name in ['bert-base-cased', 'bert-base-uncased', 'roberta-base']:\n",
    "        model = AutoModelForMaskedLM.from_pretrained(model_name, output_hidden_states=True)\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, output_hidden_states=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    return model, tokenizer, (model_name in ['bert-base-cased', 'bert-base-uncased', 'roberta-base'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b899fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_to_test = [-4, -3, -2, -1] #for time considerations we only look at ablation in the last 4 layers\n",
    "sample_output = pickle.load(open(\"wikitext_embs/{}_512.p\".format(model_name), \"rb\" ))\n",
    "sample_corpus = sample_output['corpus']\n",
    "model, tokenizer, is_mlm = load_model_tokenizer(model_name)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "#need more data for masked lm distributions since some tokens need to be preserved, \n",
    "#regardless for time considerations we limit the size of the sample\n",
    "if(is_mlm):\n",
    "    sample_for_exp = sample_corpus\n",
    "    all_toks, mask_locations = tokenize_mlm(tokenizer, sample_for_exp)\n",
    "    original_model_preds, embs = make_predictions_mlm(model, tokenizer, all_toks, mask_locations, get_embs=True)\n",
    "\n",
    "else:\n",
    "    sample_for_exp = sample_corpus[:23] #since we're only looking at masked tokens in mlm, but all tokens in clm, we truncate the text sample (23)\n",
    "    all_toks = tokenize_clm(tokenizer, sample_for_exp)\n",
    "    original_model_preds, embs = make_predictions_clm(model, tokenizer, all_toks, get_embs=True)\n",
    "    \n",
    "pickle.dump(original_model_preds, open('{}_preds.p'.format(model_name), \"wb\" )) \n",
    "pickle.dump(embs, open('{}_pred_embs.p'.format(model_name), \"wb\" )) \n",
    "\n",
    "\n",
    "model_mean_kl_ds = np.zeros((len(layers_to_test),num_dims[model_name]))\n",
    "model_std_kl_ds = np.zeros((len(layers_to_test),num_dims[model_name]))\n",
    "for layer in tqdm(layers_to_test):\n",
    "    for dim in tqdm(list(range(num_dims[model_name]))):\n",
    "        ablated_model = remove_dim_from_model_params(layer, dim, model, model_name)\n",
    "        ablated_model.to(device)\n",
    "        ablated_model.eval()\n",
    "        if(is_mlm):\n",
    "            ablated_model_preds = make_predictions_mlm(ablated_model, tokenizer, all_toks, mask_locations)\n",
    "        else:\n",
    "            ablated_model_preds = make_predictions_clm(ablated_model, tokenizer, all_toks)\n",
    "        mean_kl_d, std_kl_d = compute_avg_kl_d(ablated_model_preds, original_model_preds)\n",
    "        model_mean_kl_ds[layer, dim] = mean_kl_d\n",
    "        model_std_kl_ds[layer, dim] = std_kl_d\n",
    "out = {'model_mean_kl_ds':model_mean_kl_ds, 'model_std_kl_ds':model_std_kl_ds}\n",
    "pickle.dump(out, open('{}_kl_ds.p'.format(model_name), \"wb\" )) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfb94b9",
   "metadata": {},
   "source": [
    "### Visualize Results from sections 4.1-4.4 (Figure 1)\n",
    "Figure 1 was generated using ggplot in R. The Rmd file is provided in the github repository (in the folder ```visualize_contribution_mismatch```). Place all of the results (pickle files) from section 3.2 and section 4 into the same folder as the Rmd and run - files will render in the ```img/``` directory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096df86b",
   "metadata": {},
   "source": [
    "### Visualize results from section 4.5 (Figure 2)\n",
    "The following cell renders the figures for section 4.5, showing the distributions of values in rogue dimensions across tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff37a3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['gpt2', 'xlnet-base-cased', 'bert-base-cased', 'roberta-base']\n",
    "fig, axs = plt.subplots(7, 4, figsize=(12,16))    \n",
    "fig.tight_layout() \n",
    "for i, model_name in enumerate(models):\n",
    "    sample_output = pickle.load(open(\"wikitext_embs/{}_30.p\".format(model_name), \"rb\" ))\n",
    "    sample_data = sample_output['embeddings']\n",
    "    sample_toks = sample_output['tokens']\n",
    "    is_intial = [i % 30 == 0 for i in range(sample_data.shape[1])]\n",
    "    not_is_intial = [not x for x in is_intial]\n",
    "    is_punct = [x in {\".\", \"\\\\\", \"&\"} for x in sample_output['tokens']]\n",
    "    not_is_punct = [not (x or y) for x, y in zip(is_punct, is_intial)]\n",
    "    for layer in range(13):\n",
    "        top_5_dims = np.argsort(sample_data[layer,:,:].std(axis=0))[-1:]\n",
    "        for dim in top_5_dims:\n",
    "            tok_no_punct_dist = sample_data[layer,not_is_punct,dim]\n",
    "            tok_punct_dist = sample_data[layer,is_punct,dim]\n",
    "            tok_is_inital_dist = sample_data[layer,is_intial,dim]\n",
    "            hist_data = np.array([tok_punct_dist, tok_is_inital_dist, tok_no_punct_dist])\n",
    "            bins=np.histogram(np.hstack((tok_no_punct_dist,tok_punct_dist,tok_is_inital_dist)), bins=40)[1]\n",
    "            axs[layer,i].hist(hist_data, stacked=True, bins=40, label=(\"punctuation\",\"position 0\", \"other tokens\"))\n",
    "      \n",
    "      axs[layer,i].set_title(f'{model_name} | l:{layer} | d:{dim}')\n",
    "axs[0,0].legend()\n",
    "fig.text(0.5, 0.00, 'Acivations across tokens in dimension', ha='center')\n",
    "fig.text(-0.015, 0.5, 'count', va='center', rotation='vertical')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ea1302",
   "metadata": {},
   "source": [
    "## Reproducing Section 5\n",
    "\n",
    "To reproduce the results from section 5, simply run all of the following cells. You can download/use the precomputed embeddings referened in section 2 of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9f25fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports needed for this section, defining the sim datasets\n",
    "from glob import glob\n",
    "import pickle\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy import stats\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c09bdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load similarity datasets\n",
    "sim_filenames = ['word_sim/simlex-999/SimLex-999.txt', \n",
    "                'word_sim/simverb-3500/SimVerb-3500.txt', \n",
    "                'word_sim/wordsim-353_r/wordsim_relatedness_goldstandard.txt', \n",
    "                'word_sim/wordsim-353_s/wordsim_similarity_goldstandard.txt',\n",
    "                'word_sim/rg-65/RG65.txt']\n",
    "\n",
    "sim_lex_pairs = {}\n",
    "for sim_lex_filename in sim_filenames:\n",
    "    dataset = sim_lex_filename.split('/')[0]\n",
    "    if dataset not in sim_lex_pairs.keys():\n",
    "        sim_lex_pairs[dataset] = []\n",
    "    with open(sim_lex_filename, 'r', encoding='utf-8') as f:\n",
    "        word_pair_lines = f.readlines()\n",
    "        for line in word_pair_lines[1:]:\n",
    "            if(sim_lex_filename[:2] == 'rg'):\n",
    "                vals = line.split(';')\n",
    "            else:\n",
    "                vals = line.split()\n",
    "            word1 = vals[0].lower()\n",
    "            word2 = vals[1].lower()\n",
    "        if(sim_lex_filename[:2] == 'wo' or sim_lex_filename[:2] == 'rg'):\n",
    "            score = float(vals[2])\n",
    "        else:\n",
    "            score = float(vals[3])\n",
    "        sim_lex_pairs[dataset].append((word1,word2,score))\n",
    "\n",
    "sim_lex_pairs = {k:list(set(v)) for k,v in sim_lex_pairs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722228d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus_sample_embs(model_name):\n",
    "    all_embs_fs = glob('wordsim_embs/{}/*.p'.format(model_name))\n",
    "    all_mean_embs = []\n",
    "    for word_fname in all_embs_fs:\n",
    "        mean_emb = pickle.load(open(word_fname, 'rb'))\n",
    "        all_mean_embs.append(mean_emb)\n",
    "    return np.stack(all_mean_embs, axis=1)\n",
    "\n",
    "\n",
    "#get embedding sample for mean/std of emb space, get PCs for all-but-the-top (Mu et al. 2018)\n",
    "def get_model_mean_std_pcs(model_name):\n",
    "    corpus_sample_embeds = get_corpus_sample_embs(model_name)\n",
    "    corpus_sample_means = corpus_sample_embeds.mean(axis=1)\n",
    "    corpus_sample_stds = corpus_sample_embeds.std(axis=1)\n",
    "    num_pcs = 3\n",
    "    corpus_sample_pcs = []\n",
    "    for layer in range(corpus_sample_embeds.shape[0]):\n",
    "        layer_sample = corpus_sample_embeds[layer,:,:]\n",
    "        layer_sample = layer_sample - corpus_sample_means[layer]\n",
    "        pca = PCA(n_components=num_pcs)\n",
    "        pca.fit(layer_sample)\n",
    "        corpus_sample_pcs.append(pca.components_)   \n",
    "    return corpus_sample_means, corpus_sample_stds, corpus_sample_pcs\n",
    "\n",
    "#general-purpose similarity function, returns specified similarity resutls given some function\n",
    "#really we should be passing the sim function in as an arguement here, but I ended up implementing this with boolean fields\n",
    "def similarity(emb1, emb2, layer, means=None, stds=None, pcs=None, ustd_cosine=False, dim_rm_cosine = False, std_cosine=False, mean_sub_cosine=False, rm_pcs_cosine=False, spearman=False):\n",
    "    if(ustd_cosine):\n",
    "        return 1 - cosine(emb1, emb2)\n",
    "  \n",
    "    elif(dim_rm_cosine):\n",
    "        emb1_rm = emb1.copy()\n",
    "        emb2_rm = emb2.copy()\n",
    "        top_5_dims = np.argsort(np.abs(means[layer]))[-5:]\n",
    "        for dim in top_5_dims:\n",
    "            emb1_rm[dim] = 0\n",
    "            emb2_rm[dim] = 0\n",
    "        return 1 - cosine(emb1_rm, emb2_rm)\n",
    "  \n",
    "    elif(spearman):\n",
    "        return stats.spearmanr(emb1, emb2)[0]\n",
    "  \n",
    "    else:\n",
    "        mean_rm_emb1 = (emb1 - means[layer])\n",
    "        mean_rm_emb2 = (emb2 - means[layer]) \n",
    "    \n",
    "    if(std_cosine):\n",
    "        emb1_std = mean_rm_emb1 / stds[layer]\n",
    "        emb2_std = mean_rm_emb2 / stds[layer]\n",
    "        return 1 - cosine(emb1_std, emb2_std)\n",
    "    \n",
    "    elif(mean_sub_cosine):\n",
    "        return 1 - cosine(mean_rm_emb1, mean_rm_emb2)\n",
    "    \n",
    "    elif(rm_pcs_cosine): #all-but-the-top\n",
    "        layer_pcs = pcs[layer]\n",
    "        rm_term_1 = np.zeros(emb1.shape[0])\n",
    "        rm_term_2 = np.zeros(emb2.shape[0])\n",
    "        for pc in layer_pcs:\n",
    "            rm_term_1 += pc.dot(emb1) * pc\n",
    "            rm_term_2 += pc.dot(emb2) * pc\n",
    "        emb1_pc_rm = mean_rm_emb1 - rm_term_1\n",
    "        emb2_pc_rm = mean_rm_emb2 - rm_term_2\n",
    "        \n",
    "    return 1 - cosine(emb1_pc_rm, emb2_pc_rm)\n",
    "\n",
    "def get_decontextualized_sim(word1, word2, model_name, corpus_sample_means, corpus_sample_stds, corpus_sample_pcs):\n",
    "    if(not os.path.exists('wordsim_embs/{}/{}.p'.format(model_name, word1))):\n",
    "        return None\n",
    "    if(not os.path.exists('wordsim_embs/{}/{}.p'.format(model_name, word2))):\n",
    "        return None\n",
    "    word1_emb = pickle.load(open('wordsim_embs/{}/{}.p'.format(model_name, word1), 'rb'))\n",
    "    word2_emb = pickle.load(open('wordsim_embs/{}/{}.p'.format(model_name, word2), 'rb'))\n",
    "    layer_sims = {'ustd_cosine':[],\n",
    "                'std_cosine':[],\n",
    "                'mean_sub_cosine':[],\n",
    "                'rm_pcs_cosine':[],\n",
    "                'spearman':[]}\n",
    "    for layer in range(word1_emb.shape[0]):\n",
    "        layer_sims['ustd_cosine'].append(similarity(word1_emb[layer], word2_emb[layer], layer, ustd_cosine=True))\n",
    "        layer_sims['spearman'].append(similarity(word1_emb[layer], word2_emb[layer], layer, spearman=True))\n",
    "        layer_sims['std_cosine'].append(similarity(word1_emb[layer], word2_emb[layer], layer, means=corpus_sample_means, stds=corpus_sample_stds, std_cosine=True))\n",
    "        layer_sims['mean_sub_cosine'].append(similarity(word1_emb[layer], word2_emb[layer], layer, means=corpus_sample_means, stds=corpus_sample_stds, mean_sub_cosine=True))\n",
    "        layer_sims['rm_pcs_cosine'].append(similarity(word1_emb[layer], word2_emb[layer], layer, means=corpus_sample_means, pcs=corpus_sample_pcs, rm_pcs_cosine=True))\n",
    "\n",
    "    return layer_sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae454b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_plots_per_model(model_name, sims):\n",
    "    layerwise_sims_spearman = {'ustd_cosine':[],\n",
    "          'std_cosine':[]}\n",
    "    label_keys = {'ustd_cosine':'original',\n",
    "          'std_cosine':'standardized',\n",
    "          'mean_sub_cosine':'mean removed',\n",
    "          'rm_pcs_cosine':'abtt',\n",
    "          'spearman':'spearman'}\n",
    "    markers = ['o',\"^\",\"s\",\"D\",\"v\"]\n",
    "    plt.figure(figsize=(4,4.5))\n",
    "    ax = plt.axes() \n",
    "    for i, sim_type in enumerate(layerwise_sims_spearman.keys()):\n",
    "        for layer in range(13):\n",
    "            layer_sims = []\n",
    "            for dataset in sims.keys():\n",
    "                layer_sims.append(stats.spearmanr(sims[dataset][layer][sim_type], sims[dataset][layer]['human'])[0])\n",
    "            layerwise_sims_spearman[sim_type].append(np.array(layer_sims).mean())\n",
    "        plt.plot(layerwise_sims_spearman[sim_type], label=label_keys[sim_type], linestyle='--', marker=markers[i])\n",
    "    #plt.ylim([0, .55])\n",
    "    plt.title(model_name, fontsize=20)\n",
    "    if(model_name == 'gpt2'):\n",
    "        plt.legend(fontsize=14)\n",
    "    plt.xlabel('layer', fontsize=16)\n",
    "    plt.ylabel('Spearman\\'s rho', fontsize=16)\n",
    "    ax.xaxis.grid()\n",
    "    ax.yaxis.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1411f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_plots_per_model(model_name, sims):\n",
    "    layerwise_sims_spearman = {'ustd_cosine':[],\n",
    "          'std_cosine':[]}\n",
    "    label_keys = {'ustd_cosine':'original',\n",
    "          'std_cosine':'standardized',\n",
    "          'mean_sub_cosine':'mean removed',\n",
    "          'rm_pcs_cosine':'abtt',\n",
    "          'spearman':'spearman'}\n",
    "    markers = ['o',\"^\",\"s\",\"D\",\"v\"]\n",
    "    plt.figure(figsize=(4,4.5))\n",
    "    ax = plt.axes() \n",
    "    for i, sim_type in enumerate(layerwise_sims_spearman.keys()):\n",
    "        for layer in range(13):\n",
    "            layer_sims = []\n",
    "            for dataset in sims.keys():\n",
    "                layer_sims.append(stats.spearmanr(sims[dataset][layer][sim_type], sims[dataset][layer]['human'])[0])\n",
    "            layerwise_sims_spearman[sim_type].append(np.array(layer_sims).mean())\n",
    "        plt.plot(layerwise_sims_spearman[sim_type], label=label_keys[sim_type], linestyle='--', marker=markers[i])\n",
    "    #plt.ylim([0, .55])\n",
    "    plt.title(model_name, fontsize=20)\n",
    "    if(model_name == 'gpt2'):\n",
    "        plt.legend(fontsize=14)\n",
    "    plt.xlabel('layer', fontsize=16)\n",
    "    plt.ylabel('Spearman\\'s rho', fontsize=16)\n",
    "    ax.xaxis.grid()\n",
    "    ax.yaxis.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cd9f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_plots(model_name, sims):\n",
    "    layerwise_sims_spearman = {'ustd_cosine':[],\n",
    "          'std_cosine':[]}\n",
    "    label_keys = {'ustd_cosine':'original',\n",
    "          'std_cosine':'standardized',\n",
    "          'mean_sub_cosine':'mean removed',\n",
    "          'rm_pcs_cosine':'abtt',\n",
    "          'spearman':'spearman'}\n",
    "    markers = ['o',\"^\",\"s\",\"D\",\"v\"]\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(12,3))\n",
    "    fig.tight_layout() \n",
    "    for i, dataset in enumerate(sims.keys()):\n",
    "        for j, sim_type in enumerate(layerwise_sims_spearman.keys()):\n",
    "            layer_sims = []\n",
    "            for layer in range(13):\n",
    "                layer_sims.append(stats.spearmanr(sims[dataset][layer][sim_type], sims[dataset][layer]['human'])[0])\n",
    "            axs[i].plot(layer_sims, label=label_keys[sim_type], linestyle='--', marker=markers[j])\n",
    "     \n",
    "        axs[i].set_title(dataset, fontsize=18)\n",
    "        axs[i].xaxis.grid()\n",
    "        axs[i].yaxis.grid()\n",
    "        axs[i].set_xticks(range(0,13,2))\n",
    "    axs[0].legend(fontsize=11)\n",
    "    fig.text(.5, 1.05, model_name, ha='center', fontsize=24)\n",
    "    fig.text(0.5, 0.00, 'layer', ha='center', fontsize=16)\n",
    "    fig.text(-0.015, 0.5, 'Spearman\\'s rho', va='center', rotation='vertical', fontsize=16)\n",
    "\n",
    "    plt.show()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68eb775b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate the plots!\n",
    "models = ['gpt2', 'xlnet-base-cased', 'bert-base-cased', 'roberta-base']\n",
    "\n",
    "for model_name in models:\n",
    "    corpus_sample_means, corpus_sample_stds, corpus_sample_pcs = get_model_mean_std_pcs(model_name)\n",
    "  \n",
    "    excl = 0\n",
    "    dataset_sims = {}\n",
    "    for sim_dataset, sim_list in sim_lex_pairs.items():\n",
    "        layerwise_decontextual_sims = [{'ustd_cosine':[],\n",
    "                'std_cosine':[],\n",
    "                'mean_sub_cosine':[],\n",
    "                'rm_pcs_cosine':[],\n",
    "                'spearman':[],\n",
    "                'human':[]} for x in range(13)]\n",
    "    \n",
    "        for sim_pair in tqdm(sim_list):\n",
    "            human_score = sim_pair[2]\n",
    "            word1 = sim_pair[0]\n",
    "            word2 = sim_pair[1]\n",
    "            decontextual_sim_score = get_decontextualized_sim(word1, word2, model_name, corpus_sample_means, corpus_sample_stds, corpus_sample_pcs)\n",
    "      \n",
    "            if(decontextual_sim_score is not None):\n",
    "                for sim_type, layer_scores in decontextual_sim_score.items():\n",
    "                    for layer_ind, layer_score in enumerate(layer_scores):\n",
    "                        layerwise_decontextual_sims[layer_ind][sim_type].append(layer_score)\n",
    "                for layer_ind in range(13):\n",
    "                    layerwise_decontextual_sims[layer_ind]['human'].append(human_score)\n",
    "            else:\n",
    "                excl += 1\n",
    "        dataset_sims[sim_dataset] = layerwise_decontextual_sims   \n",
    "    \n",
    "    generate_plots(model_name, dataset_sims)\n",
    "    print(excl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
